# Q&A Chatbot with File Upload

A powerful multi-user chatbot that allows users to upload PDF files and ask questions about their content using Gemini AI and HuggingFace embeddings with optional GPU acceleration.

## âœ¨ Features

- **Multi-User Support**: Create profiles and manage separate document collections
- **PDF Processing**: Upload and extract text from PDF documents
- **RAG-based Q&A**: Intelligent question answering using Retrieval-Augmented Generation
- **Privacy Options**: Choose between cloud (Gemini) or local (Ollama) AI models
- **Duplicate Handling**: Smart duplicate detection with replace/update options
- **Conversation Memory**: Maintains chat history across sessions
- **GPU Acceleration**: Automatic GPU detection for faster embeddings (optional)
- **Document Management**: View, switch between, and delete uploaded PDFs
- **Persistent Storage**: All data saved locally with automatic loading

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Set up AI Model

**Option A: Cloud AI (Gemini)**
Create a `.env` file in the project directory:
```env
GEMINI_API_KEY=your_actual_api_key_here
```

**Option B: Local AI (Ollama) - For Maximum Privacy**
```bash
# Install Ollama from https://ollama.com/download

# Pull tinyllama model (default model used by the app)
ollama pull tinyllama
```

### 3. Run the Application
```bash
streamlit run app.py
```

### 4. Start Using
1. Create a user profile or select existing one
2. Choose your AI model (Cloud/Local) in the sidebar
3. Upload a PDF file using the sidebar
4. Ask questions about the uploaded content
5. Switch between different PDFs anytime

## ğŸ”§ Optional GPU Setup

For faster embedding processing on GPU-enabled systems:

```bash
# Install PyTorch with CUDA support
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

```

The app automatically detects and uses GPU when available, showing "ğŸ–¥ï¸ GPU Embeddings" in the user profile.

## ğŸ“ Project Structure

```text
AI-DocTalk/
â”‚
â”œâ”€â”€ app.py                    # Main Streamlit application
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ README.md                # Project documentation
â”œâ”€â”€ .env                     # Environment variables (create this)
â”œâ”€â”€ users.json              # User profiles (auto-created)
â”‚
â””â”€â”€ user_data/              # User documents and embeddings
    â””â”€â”€ {username}/
        â”œâ”€â”€ {pdf_name}_embeddings/
        â”‚   â”œâ”€â”€ index.faiss     # FAISS vector index
        â”‚   â””â”€â”€ index.pkl       # Metadata
        â””â”€â”€ {pdf_name}_chat.json # Chat history
```

## ğŸ¯ Usage Guide

### Creating Users
- First time: Enter your name to create a profile
- Returning: Select your profile from the dropdown

### Managing PDFs
- **Upload**: Use sidebar file uploader
- **Duplicate**: Choose "Replace/Update" or "Cancel"
- **Switch**: Click any PDF from "Your Documents" list
- **Delete**: Use ğŸ—‘ï¸ button next to each PDF

### Chatting
- Ask questions about the active PDF content
- View conversation history
- Clear chat history with "Clear Chat" button
- Questions unrelated to PDF content are filtered out

## ğŸ› ï¸ Technical Details

- **Frontend**: Streamlit web interface
- **AI Models**: 
  - ğŸŒ Google Gemini 1.5 Flash (Cloud)
  - ğŸ”’ Ollama (Local: tinyllama)
- **Embeddings**: HuggingFace sentence-transformers/all-MiniLM-L6-v2
- **Vector Store**: FAISS for similarity search
- **Text Processing**: RecursiveCharacterTextSplitter for optimal chunking
- **Storage**: Local JSON and FAISS files

## ğŸ”’ Data Privacy

**Local Mode (Ollama):**
- ğŸ”’ **Complete Privacy**: All processing happens locally
- ğŸš« **No External Calls**: Zero data sent to external services
- ğŸ’¾ **Local Storage**: Documents and chat history stay on your machine

**Cloud Mode (Gemini):**
- ğŸ“ **Local Storage**: Documents stored locally
- ğŸŒ **API Calls**: Only questions and context sent to Gemini API
- ğŸ” **No Document Upload**: Full PDFs never leave your machine

## ğŸ› Troubleshooting

**"No text found in PDF"**: PDF might be image-based or corrupted
**"Please set GEMINI_API_KEY"**: Check your .env file (only needed for Cloud mode)
**"Failed to connect to Ollama"**: Make sure Ollama is running and model is installed
**"Failed to load embeddings"**: Try re-uploading the PDF
**Slow processing**: Consider GPU setup for faster embeddings

## ğŸ“‹ Requirements

- Python 3.8+
- **For Cloud Mode**: Gemini API key
- **For Local Mode**: Ollama installation
- 2GB+ RAM for embeddings
- Optional: CUDA-compatible GPU for acceleration

## ğŸ¤ Contributing

Feel free to submit issues and enhancement requests!
